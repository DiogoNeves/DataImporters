{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'/home/diogoneves/Projects/metaphora/DataImporters'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#hide\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "\n",
    "if os.getcwd().endswith('nbs'):\n",
    "    os.chdir(\"..\")\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Audio Data Importers\n",
    "\n",
    "> A collection of data importers for various audio sources. A loose manual data pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`pip install dataimporters`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Downloading Audio Sources\n",
    "\n",
    "The audio sources have to be provided manually (for now).  \n",
    "The scripts expect a data directory containing the audio folders:  \n",
    "```\n",
    "root\n",
    " |- data\n",
    "      |- original (where you have to place the soundbanks)\n",
    "      |- intermediate (generated)\n",
    "      |- dataset (generated)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to use"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To create a new dataset package, we simply:  \n",
    "1. import the `Dataset`,  \n",
    "1. give it the sources we'd like to include and the path to our data,  \n",
    "1. call `Dataset.compile`\n",
    "\n",
    "The library is flexible, but here's the simplest and most common action we perform:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "zip error: Nothing to do! (try: zip -qq -FSr dataset.zip . -i data/dataset/)\n"
     ]
    }
   ],
   "source": [
    "#hide_output\n",
    "from DataImporters.dataset import Dataset\n",
    "\n",
    "DATA_PATH = \"data/\"\n",
    "SOURCES = [\n",
    "    \"space_divers_mini\",\n",
    "    \"footsteps_one_ppsfx_004\",\n",
    "    \"footsteps_two_ppsfx_008\",\n",
    "    \"edward_v1.1\"\n",
    "]\n",
    "\n",
    "metadata = Dataset(SOURCES, DATA_PATH).compile()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Dataset.compile` will return the newly created metadata _(which has already been saved to `DATA_PATH`)_.  \n",
    "\n",
    "We can use it to confirm we did indeed copy all files. Since the metadata aggregates all the source metadata, if a file is missing, it will still be in the metadata.  \n",
    "On the other hand, this will also let us know when a file has been deleted from the source, but still exists in the dataset folder.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "assert len(os.listdir(os.path.join(DATA_PATH, \"dataset/audio/\"))) == len(metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the assertion fails, this could be due to:  \n",
    "* Genuine failure to copy  \n",
    "* Some files in the target folder need deleting  \n",
    "  * Please delete them, no code yet\n",
    "* Hash conflict (same content from different sources)  \n",
    "  * In this case, we must debug the sources and make sure there are no duplicates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset structure\n",
    "\n",
    "See [Dataset README](data/dataset/README.md)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline\n",
    "\n",
    "```mermaid\n",
    "flowchart TD\n",
    "    sa[(Source A)] --> pa([Normalise data and create CSV]);\n",
    "    pa --> ia[(Intermediate A)];\n",
    "    sb[(Source B)] --> pb([Normalise data and create CSV]);\n",
    "    pb --> ib[(Intermediate B)];\n",
    "    ia & ib & a(WIP: Manual annotations by hash) --> c([Compile])\n",
    "    c-- Some rows can be rejected at this stage --> d[(Dataset)];\n",
    "```\n",
    "\n",
    "Each loader outputs:  \n",
    "* a CSV, which is then compiled into a single metadata.csv  \n",
    "* the files into an intermediate folder  \n",
    "\n",
    "The process above is done so that:  \n",
    "* Each notebook is independent  \n",
    "* We can easily compile a final dataset with different sources  \n",
    "* Easier to make the split consistent across runs  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('.venv': venv)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
