{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'/home/diogoneves/Projects/metaphora/DataImporters'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#hide\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "\n",
    "if os.getcwd().endswith('nbs'):\n",
    "    os.chdir(\"..\")\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Audio Data Importers\n",
    "\n",
    "> A collection of data importers for various audio sources. A loose manual data pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```bash\n",
    "pip install dataimporters\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Downloading Audio Sources\n",
    "\n",
    "The audio sources have to be provided manually (for now).  \n",
    "The scripts expect a data directory containing the audio folders:  \n",
    "```\n",
    "root\n",
    " |- data/\n",
    "      |- original/ (where you have to place the soundbanks)\n",
    "      |- intermediate/ (generated)\n",
    "      |- dataset/ (generated)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contributing\n",
    "\n",
    "We use **nbdev**, which compiles all notebooks into a package. The source is at the `nbs` folder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to Use"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To create a new dataset package, we simply:  \n",
    "1. Define and process all sources,\n",
    "1. import the `Dataset`,  \n",
    "1. give it the sources we'd like to include and the path to our data,  \n",
    "1. call `Dataset.compile`\n",
    "\n",
    "This will process all sources and build a final `dataset.zip` file.  \n",
    "\n",
    "The library is flexible, but here's the simplest and most common action we perform:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For annotations, see `nbs/12_review.ipynb` notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#hide_ouput\n",
    "from DataImporters.core import load_version\n",
    "\n",
    "DATA_DIR = \"data/\"\n",
    "VERSION = load_version()\n",
    "VERSION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from DataImporters.sources.core import process\n",
    "from DataImporters.sources.space_divers_mini import SpaceDiversMini\n",
    "from DataImporters.sources.footsteps_one_ppsfx import FootstepsOnePpsfx\n",
    "from DataImporters.sources.footsteps_two_ppsfx import FootstepsTwoPpsfx\n",
    "from DataImporters.sources.edward import Edward\n",
    "from DataImporters.sources.barefoot_metal_sonniss import BarefootMetalSonniss\n",
    "from DataImporters.sources.custom_fsd import CustomFsd\n",
    "\n",
    "all_sources = [\n",
    "    SpaceDiversMini(),\n",
    "    FootstepsOnePpsfx(),\n",
    "    FootstepsTwoPpsfx(),\n",
    "    Edward(),\n",
    "    BarefootMetalSonniss(),\n",
    "    CustomFsd()\n",
    "]\n",
    "\n",
    "for source in all_sources:\n",
    "    process(source, DATA_DIR, VERSION)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Below are two examples, one creates a large dataset with the automatic processors, the other is a more balanced dataset, manually annotated.  \n",
    "Choose one to run and then jump to `Verify Output`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Larger Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: 206 duplicate rows found. Some rows were dropped (all files copied).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1646"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from DataImporters.dataset import Dataset, DatasetPaths\n",
    "\n",
    "DATASET_NAME = \"large\"\n",
    "\n",
    "# Same as `all_sources` excluding SpaceDiversMini\n",
    "sources = [\n",
    "    FootstepsOnePpsfx(),\n",
    "    FootstepsTwoPpsfx(),\n",
    "    Edward(),\n",
    "    BarefootMetalSonniss(),\n",
    "    CustomFsd()\n",
    "]\n",
    "\n",
    "paths = DatasetPaths(DATA_DIR, DATASET_NAME)\n",
    "metadata = Dataset(sources, paths).compile()\n",
    "metadata.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Smaller and Annotated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: 207 duplicate rows found. Some rows were dropped (all files copied).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "284"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from DataImporters.dataset import Dataset, DatasetPaths\n",
    "\n",
    "DATASET_NAME = \"small_balanced\"\n",
    "ANNOTATION_PATH = os.path.join(DATA_DIR, \"annotations\", DATASET_NAME + \".csv\")\n",
    "\n",
    "sources = [\n",
    "    CustomFsd()\n",
    "]\n",
    "\n",
    "paths = DatasetPaths(DATA_DIR, DATASET_NAME, ANNOTATION_PATH)\n",
    "metadata = Dataset(sources, paths).compile()\n",
    "metadata.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verify Output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Dataset.compile` will return the newly created metadata _(which has already been saved to `DATA_PATH`)_.  \n",
    "\n",
    "We can use it to confirm we did indeed copy all files. Since the metadata aggregates all the source metadata, if a file is missing, it will still be in the metadata.  \n",
    "On the other hand, this will also let us know when a file has been deleted from the source, but still exists in the dataset folder.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "assert len(os.listdir(paths.audio_output_path)) == len(metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Everything is looking good, we should bump the `version`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Version moved from 17 to 18\n"
     ]
    }
   ],
   "source": [
    "#hide_output\n",
    "\n",
    "from DataImporters.core import bump_version\n",
    "bump_version()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the assertion fails, this could be due to:  \n",
    "* Genuine failure to copy  \n",
    "* Some files in the target folder need deleting  \n",
    "  * Please delete them, no code yet\n",
    "* Hash conflict (same content from different sources)  \n",
    "  * In this case, we must debug the sources and make sure there are no duplicates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Structure\n",
    "\n",
    "```\n",
    "dataset/\n",
    "  |- README.md\n",
    "  |- metadata.csv\n",
    "  |- audio/\n",
    "       |- Long list of audio files, filenames are the xxhash64 of the content.\n",
    "```\n",
    "\n",
    "### Metadata\n",
    "\n",
    "`metadata.csv` contains a list of all the files in the dataset and their labels.  \n",
    "\n",
    "filename | category | label | extra | source | version \n",
    "--- | --- | --- | --- | --- | --- \n",
    "File name, assumes all files inside audio folder | Single major category name | Escaped (“”) comma separated list of labels, in snake_case | Extra text/details available for this row (unstructured) | Name of original sound library, snake_case | Version of the last change. Limited to last change only  \n",
    "\n",
    "_`version` is a simple incremental integer. If you need to check if a file changed/added simply check if the row `version` is higher than the last `version` you ran. Deletes are not supported yet._\n",
    "\n",
    "Here's an example from the sample code ran earlier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>category</th>\n",
       "      <th>label</th>\n",
       "      <th>extra</th>\n",
       "      <th>source</th>\n",
       "      <th>version</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>a2abde886f52e83e.wav</td>\n",
       "      <td>Zombie_noises</td>\n",
       "      <td>Horror</td>\n",
       "      <td>NaN</td>\n",
       "      <td>custom_fsd</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>af864f34b0f0c323.wav</td>\n",
       "      <td>Zombie_noises</td>\n",
       "      <td>Scary,Growl,Monster,Zombie,Female,Scream</td>\n",
       "      <td>NaN</td>\n",
       "      <td>custom_fsd</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               filename       category  \\\n",
       "0  a2abde886f52e83e.wav  Zombie_noises   \n",
       "1  af864f34b0f0c323.wav  Zombie_noises   \n",
       "\n",
       "                                      label  extra      source  version  \n",
       "0                                    Horror    NaN  custom_fsd       16  \n",
       "1  Scary,Growl,Monster,Zombie,Female,Scream    NaN  custom_fsd       16  "
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#hide_input\n",
    "metadata.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline\n",
    "\n",
    "```mermaid\n",
    "flowchart TD\n",
    "    sa[(Source A)] --> pa([Normalise data and create CSV]);\n",
    "    pa --> ia[(Intermediate A)];\n",
    "    sb[(Source B)] --> pb([Normalise data and create CSV]);\n",
    "    pb --> ib[(Intermediate B)];\n",
    "    ia & ib & a(WIP: Manual annotations by hash) --> c([Compile])\n",
    "    c-- Some rows can be rejected at this stage --> d[(Dataset)];\n",
    "```\n",
    "\n",
    "Each loader outputs:  \n",
    "* a CSV, which is then compiled into a single metadata.csv  \n",
    "* the files into an intermediate folder  \n",
    "\n",
    "The process above is done so that:  \n",
    "* Each source is independent  \n",
    "* We can easily compile a final dataset with different sources  \n",
    "* Easier to make the split consistent across runs  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
